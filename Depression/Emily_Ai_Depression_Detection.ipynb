{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f1LRd9W1Yfe"
      },
      "source": [
        "# Emily Ai Model Training\n",
        "Depression Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zHEcqlvz15u2"
      },
      "outputs": [],
      "source": [
        "# All required imports\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fvdoXV61xiI"
      },
      "source": [
        "# Preprocessing the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PRU3fm_n1UoP"
      },
      "outputs": [],
      "source": [
        "def preprocess(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Convert from float64 to float32 and normalize normalize to decibels\n",
        "    relative to full scale (dBFS) for the 4 sec clip.\n",
        "    \"\"\"\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train = np.array([(X - X.min()) / (X.max() - X.min()) for X in X_train])\n",
        "    X_test = np.array([(X - X.min()) / (X.max() - X.min()) for X in X_test])\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH4cuyWi2Jce"
      },
      "source": [
        "# Train Test divide and Image Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7k_bzLMp2IHG"
      },
      "outputs": [],
      "source": [
        "def prep_train_test(X_train, y_train, X_test, y_test, nb_classes):\n",
        "    \"\"\"\n",
        "    Prep samples ands labels for Keras input by noramalzing and converting\n",
        "    labels to a categorical representation.\n",
        "    \"\"\"\n",
        "    print('Train on {} samples, validate on {}'.format(X_train.shape[0],\n",
        "                                                       X_test.shape[0]))\n",
        "\n",
        "    # normalize to dBfS\n",
        "    X_train, X_test = preprocess(X_train, X_test)\n",
        "\n",
        "    # Convert class vectors to binary class matrices\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def keras_img_prep(X_train, X_test, img_dep, img_rows, img_cols):\n",
        "    \"\"\"\n",
        "    Reshape feature matrices for Keras' expexcted input dimensions.\n",
        "    For 'th' (Theano) dim_order, the model expects dimensions:\n",
        "    (# channels, # images, # rows, # cols).\n",
        "    \"\"\"\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "    return X_train, X_test, input_shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dshrwdW2rzX"
      },
      "source": [
        "# Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n6KpT-OQT28y"
      },
      "outputs": [],
      "source": [
        "def cnn(X_train, y_train, X_test, y_test, batch_size,\n",
        "        nb_classes, epochs, input_shape):\n",
        "  weights = {\n",
        "          0: np.random.rand(),\n",
        "          1: np.random.rand()\n",
        "        }\n",
        "  # path = r\"/content/drive/MyDrive/Emily\"\n",
        "  checkpoint_filepath = r'./weights-{epoch:02d}-acc{val_accuracy:.4f}-loss{val_loss:.4f}.h5'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "  model = Sequential()\n",
        "  # model.add(Conv2D(32, (5,5), activation='relu', input_shape=input_shape))\n",
        "  # model.add(MaxPooling2D((4, 4), strides=4))\n",
        "  # model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  # model.add(MaxPooling2D((1, 3), strides=(1,3)))\n",
        "  # model.add(Flatten())\n",
        "  # model.add(Dense(128, activation='linear'))\n",
        "  # model.add(Dropout(0.6))\n",
        "  # model.add(Dense(256, activation='relu'))\n",
        "  # model.add(Dropout(0.8))\n",
        "  # model.add(Dense(2, activation='sigmoid'))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  # model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "  # model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(2, activation='sigmoid'))\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adagrad(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        verbose=1, \n",
        "                        validation_data=(X_test, y_test),\n",
        "                        shuffle=True,\n",
        "                        callbacks=[model_checkpoint_callback],\n",
        "                        # validation_split=0.1\n",
        "                        class_weight={0:np.random.rand(),1:np.random.rand()}, \n",
        "                        )\n",
        "  return model, history\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ark_dZWg2yXX"
      },
      "source": [
        "# Main Runner Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YBz3gQ-vgZSq"
      },
      "outputs": [],
      "source": [
        "# Loading Dataset\n",
        "# path = r\"/content/drive/MyDrive/Emily\"\n",
        "\n",
        "# Loading Dataset\n",
        "X_train = np.load(\n",
        "        r'./train_samples_8sec.npz')\n",
        "y_train = np.load(\n",
        "        './train_labels_8sec.npz')\n",
        "X_test = np.load(\n",
        "        './test_samples_8sec.npz')\n",
        "y_test = np.load(\n",
        "    './test_labels_8sec.npz')\n",
        "\n",
        "X_train, y_train, X_test, y_test = \\\n",
        "        X_train['arr_0'], y_train['arr_0'], X_test['arr_0'], y_test['arr_0']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcIyfurO26OT",
        "outputId": "8aa6f8da-7cce-4012-adb8-ae8f7c446cff"
      },
      "outputs": [],
      "source": [
        "# Hyper Parameters\n",
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "epochs = 100\n",
        "# Train Test Size Update\n",
        "X_train, X_test, y_train, y_test = prep_train_test(X_train, y_train,\n",
        "                                                       X_test, y_test,\n",
        "                                                       nb_classes=nb_classes)\n",
        "\n",
        "# 513x125x1 for spectrogram with crop size of 125 pixels\n",
        "img_rows, img_cols, img_depth = X_train.shape[1], X_train.shape[2], 1\n",
        "\n",
        "# Spectogram Image Processing\n",
        "X_train, X_test, input_shape = keras_img_prep(X_train, X_test, img_depth,\n",
        "                                                  img_rows, img_cols)\n",
        "\n",
        "# print(y_test)\n",
        "# print(y_train)\n",
        "\n",
        "# Loaing and Fitting The model\n",
        "model, history = cnn(X_train, y_train, X_test, y_test, batch_size,\n",
        "                         nb_classes, epochs, input_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPV609PNK0tS",
        "outputId": "5e8ccc48-a6b5-44a1-b92d-7a7828d8f2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72/72 [==============================] - 3s 36ms/step - loss: 0.9373 - accuracy: 0.5000\n",
            "Train accuracy: 0.5\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.9381 - accuracy: 0.5000\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "score_train = model.evaluate(X_train, y_train, verbose=1)\n",
        "print('Train accuracy:', score_train[1])\n",
        "score_test = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test accuracy:', score_test[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Emily Ai Depression Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
